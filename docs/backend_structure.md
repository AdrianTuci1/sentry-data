# Sentry Agentic Backend Structure

## Overview
The backend is the brain of the Sentry Agentic Service. It is built with **FastAPI** (Python) to leverage the rich AI/ML ecosystem. It acts as an orchestrator between the user (Frontend), the "Memory" (Lakehouse), the "Workspace" (E2B), and the "Muscle" (Modal).

## Core Tech Stack
- **Framework:** FastAPI (Python 3.10+)
- **Database:**
  - **Metadata:** PostgreSQL (via Supabase) - Users, Projects, Model Configs, Cost Logs.
  - **Analytics/Data:** DuckDB - In-memory SQL engine for querying S3 parquet/csv files efficiently.
- **Object Storage:** AWS S3 / Cloudflare R2 - Raw data lake.
- **Agent Sandbox:** E2B SDK - For running code interpretation and data manipulation securely.
- **Compute:** Modal Client - For dispatching heavy training jobs to GPUs.
- **Auth:** Supabase Auth / JWT.

## Directory Structure
```
sentry-backend/
├── app/
│   ├── api/
│   │   ├── endpoints/
│   │   │   ├── auth.py         # User authentication
│   │   │   ├── chat.py         # Chat session management
│   │   │   ├── files.py        # S3/Lakehouse file operations
│   │   │   ├── models.py       # Model registry & metrics
│   │   │   └── workflows.py    # Trigger training/inference jobs
│   │   └── websocket.py        # Real-time communication manager
│   ├── core/
│   │   ├── config.py           # Env vars, settings
│   │   ├── security.py         # JWT handling
│   │   └── database.py         # DB connection session
│   ├── services/
│   │   ├── agent/
│   │   │   ├── planner.py      # LLM logic to creating plans
│   │   │   └── executor.py     # Logic to run steps
│   │   ├── connectors/
│   │   │   ├── s3_connector.py # Boto3/S3 wrapper
│   │   │   └── duckdb_client.py# DuckDB query wrapper
│   │   ├── e2b_client.py       # Wrapper for E2B sandbox management
│   │   └── modal_client.py     # Wrapper for dispatching Modal jobs
│   ├── models/                 # SQLAlchemy / Pydantic models
│   │   ├── user.py
│   │   ├── session.py
│   │   └── ml_model.py
│   ├── schemas/                # Pydantic schemas for Request/Response
│   └── main.py                 # App entry point
├── Dockerfile
├── requirements.txt
├── .env.example
└── alembic/                    # DB Migrations (if using SQL or similar)
```

## Key Modules

### 1. Lakehouse Integration (The Memory)
- **Scanning:** `s3_connector.py` lists buckets and files.
- **Discovery:** `duckdb_client.py` samples CSV/Parquet files to infer schema and statistics (min/max, nulls) without downloading the whole dataset.
- **API:** `GET /files/scan?path=s3://...` returns a file tree with metadata.

### 2. E2B Integration (The Workspace)
- **Role:** When the user asks to "Analyze this data", the backend spins up an E2B sandbox.
- **Process:**
  1. Backend creates E2B sandbox.
  2. Uploads snippet or mounts S3 bucket (via signed URLs).
  3. Executes Python code generated by the Agent (LLM).
  4. Streams `stdout`/`stderr` back to Frontend via WebSocket.
  5. Returns artifacts (images, cleaned csvs) to S3.

### 3. Modal Integration (The Muscle)
- **Role:** Heavy training that requires GPUs.
- **Process:**
  1. Agent prepares a `train.py` script and `requirements.txt`.
  2. Backend calls a remote Modal function `submit_job(script, resource_reqs)`.
  3. Modal spins up A100.
  4. Modal creates a checkpoint/model file in S3.
  5. Backend tracks job status and cost (start_time * rate).

### 4. Agent Architecture
- **Planner:** Receives user prompt. Uses LLM (e.g., GPT-4o or Claude 3.5 Sonnet) to break down task into steps:
  - Step 1: Scan data.
  - Step 2: Clean nulls.
  - Step 3: Train Random Forest.
- **Executor:** Executes steps sequentially, asking for user confirmation when `requires_approval=True` (e.g., before spending calculation credits).

## Data Flow
1. **User** sends prompt "Train a model on sales.csv" via **WebSocket**.
2. **Backend** Agent checks **Connectors** (S3).
3. **Backend** uses **DuckDB** to get schema of `sales.csv`.
4. **Backend** generates a plan and sends "Approval Request" with "Estimated Cost" to **Frontend**.
5. **User** clicks "Proceed".
6. **Backend** triggers **E2B** to preprocess data.
7. **Backend** triggers **Modal** to train model.
8. **Frontend** updates "Launched Models" sidebar with new entry "Sales Predictor v1" (Training...).
